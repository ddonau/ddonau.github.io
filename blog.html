<!DOCTYPE html>
<html>
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Introduction to Python Ensembles</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.8.4/themes/prism.min.css" rel="stylesheet" />
    <link rel="stylesheet" type="text/css" href="/blog/assets/built/screen.css?v=85bbdda1fa" />
    <link rel="stylesheet" type="text/css" href="/blog/assets/built/custom.css?v=85bbdda1fa" />

        <meta name="description" content="This post takes you through the basics of ensembles — what they are and why they work so well — and provides a hands-on tutorial for building basic ensembles." />
    <link rel="shortcut icon" href="/blog/favicon.png" type="image/png" />
    <link rel="canonical" href="https://www.dataquest.io/blog/introduction-to-ensembles/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Dataquest" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Introduction to Python Ensembles" />
    <meta property="og:description" content="This post takes you through the basics of ensembles — what they are and why they work so well — and provides a hands-on tutorial for building basic ensembles." />
    <meta property="og:url" content="https://www.dataquest.io/blog/introduction-to-ensembles/" />
    <meta property="og:image" content="https://www.dataquest.io/blog/content/images/2018/01/python-ensembles-1.png" />
    <meta property="article:published_time" content="2018-01-11T18:51:04.000Z" />
    <meta property="article:modified_time" content="2018-04-26T21:11:05.000Z" />
    <meta property="article:tag" content="Python" />
    <meta property="article:tag" content="Data Science" />
    
    <meta property="article:publisher" content="https://www.facebook.com/dataquestio/" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Introduction to Python Ensembles" />
    <meta name="twitter:description" content="Learn the basics of #python ensembles and build your own, in our newest tutorial. #datascience" />
    <meta name="twitter:url" content="https://www.dataquest.io/blog/introduction-to-ensembles/" />
    <meta name="twitter:image" content="https://www.dataquest.io/blog/content/images/2018/01/python-ensembles.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Flennerhag" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Python, Data Science" />
    <meta name="twitter:site" content="@dataquestio" />
    <meta name="twitter:creator" content="@flennerhag" />
    <meta property="og:image:width" content="1024" />
    <meta property="og:image:height" content="512" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Dataquest",
        "logo": "https://www.dataquest.io/blog/content/images/2017/12/Logo-On-Dark--2-.png"
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Flennerhag",
        "image": {
            "@type": "ImageObject",
            "url": "https://www.dataquest.io/blog/content/images/2018/01/Sebastian_Flennerhag-1.jpg",
            "width": 3205,
            "height": 4807
        },
        "url": "https://www.dataquest.io/blog/author/sebastian/",
        "sameAs": [
            "http://flennerhag.com",
            "https://twitter.com/flennerhag"
        ]
    },
    "headline": "Introduction to Python Ensembles",
    "url": "https://www.dataquest.io/blog/introduction-to-ensembles/",
    "datePublished": "2018-01-11T18:51:04.000Z",
    "dateModified": "2018-04-26T21:11:05.000Z",
    "keywords": "Python, Data Science",
    "description": "This post takes you through the basics of ensembles — what they are and why they work so well — and provides a hands-on tutorial for building basic ensembles.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://www.dataquest.io/blog/"
    }
}
    </script>

    <script src="/blog/public/ghost-sdk.min.js?v=85bbdda1fa"></script>
<script>
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "2e84fde6d9c4"
});
</script>
    <meta name="generator" content="Ghost 1.24" />
    <meta property="fb:pages" content="937097596357357" />


</head>
<body class="post-template tag-python tag-data-science">

    <div class="site-wrapper">

        


<header class="site-header outer">
</header>


<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post tag-python tag-data-science no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="2018-01-11">11 January 2018</time>
                        <span class="date-divider">/</span> <a href="/blog/tag/python/">Python</a>
                </section>
                <h1 class="post-full-title">Introduction to Python Ensembles</h1>
            </header>


            <section class="post-full-content">
                <div class="kg-card-markdown"><h2 id="stackingmodelsinpythonefficiently">Stacking models in Python efficiently</h2>
<p>Ensembles have rapidly become one of the hottest and most popular methods in applied machine learning. Virtually <a href="http://blog.kaggle.com/category/winners-interviews/">every winning Kaggle solution</a> features them, and many data science pipelines have ensembles in them.</p>
<p>Put simply, ensembles combine predictions from different models to generate a final prediction, and the more models we include the better it performs. Better still, because ensembles combine baseline predictions, they perform at least as well as the best baseline model. Ensembles give us a performance boost almost for free!</p>
<p><img src="/blog/content/images/2018/01/network-1.png" alt="network"><br>
<em>Example schematics of an ensemble. An input array \(X\) is fed through two preprocessing pipelines and then to a set of base learners \(f^{(i)}\). The ensemble combines all base learner predictions into a final prediction array \(P\). <a href="http://ml-ensemble.com/">Source</a></em></p>
<p>In this post, we'll take you through the basics of ensembles — what they are and why they work so well — and provide a hands-on tutorial for building basic ensembles. By the end of this post, you will:</p>
<ul>
<li>understand the fundamentals of ensembles</li>
<li>know how to code them</li>
<li>understand the main pitfalls and drawbacks of ensembles</li>
</ul>
<h2 id="predictingrepublicananddemocraticdonations">Predicting Republican and Democratic donations</h2>
<p>To illustrate how ensembles work, we'll use a data set on U.S. political contributions.  The <a href="https://github.com/fivethirtyeight/data/tree/master/science-giving">original data set</a> was prepared by <a href="https://fivethirtyeight.com/contributors/ben-wieder/">Ben Wieder</a> at <a href="https://fivethirtyeight.com/">FiveThirtyEight</a>, who dug around the U.S. government's political contribution registry and found that when <a href="https://fivethirtyeight.com/features/when-scientists-donate-to-politicians-its-usually-to-democrats/">scientists donate to politician, it's usually to Democrats</a>.</p>
<p>This claim is based on the observation on the share of donations being made to Republicans and Democrats. However, there's plenty more that can be said: for instance, which scientific discipline is most likely to make a Republican donation, and which state is most likely to make Democratic donations? We will go one step further and <em>predict</em> whether a donation is most likely to be a to a Republican or Democrat.</p>
<p>The <a href="https://www.dataquest.io/blog/large_files/input.csv">data</a> we use here is slightly adapted. We remove any donations to party affiliations other than Democrat or Republican to make our exposition a little clearer and drop some duplicate and less interesting features. The data script can be found <a href="https://www.dataquest.io/blog/large_files/gen_data.py">here</a>. Here's the data:</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline

### Import data
# Always good to set a seed for reproducibility
SEED = 222
np.random.seed(SEED)

df = pd.read_csv('input.csv')

### Training and test set
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

def get_train_test(test_size=0.95):
    &quot;&quot;&quot;Split Data into train and test sets.&quot;&quot;&quot;
    y = 1 * (df.cand_pty_affiliation == &quot;REP&quot;)
    X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1)
    X = pd.get_dummies(X, sparse=True)
    X.drop(X.columns[X.std() == 0], axis=1, inplace=True)
    return train_test_split(X, y, test_size=test_size, random_state=SEED)

xtrain, xtest, ytrain, ytest = get_train_test()

# A look at the data
print(&quot;\nExample data:&quot;)
df.head()
</code></pre>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }
<pre><code>.dataframe thead th {
    text-align: left;
}

.dataframe tbody tr th {
    vertical-align: top;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cand_pty_affiliation</th>
      <th>cand_office_st</th>
      <th>cand_office</th>
      <th>cand_status</th>
      <th>rpt_tp</th>
      <th>transaction_tp</th>
      <th>entity_tp</th>
      <th>state</th>
      <th>classification</th>
      <th>cycle</th>
      <th>transaction_amt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>REP</td>
      <td>US</td>
      <td>P</td>
      <td>C</td>
      <td>Q3</td>
      <td>15</td>
      <td>IND</td>
      <td>NY</td>
      <td>Engineer</td>
      <td>2016.0</td>
      <td>500.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>DEM</td>
      <td>US</td>
      <td>P</td>
      <td>C</td>
      <td>M5</td>
      <td>15E</td>
      <td>IND</td>
      <td>OR</td>
      <td>Math-Stat</td>
      <td>2016.0</td>
      <td>50.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DEM</td>
      <td>US</td>
      <td>P</td>
      <td>C</td>
      <td>M3</td>
      <td>15</td>
      <td>IND</td>
      <td>TX</td>
      <td>Scientist</td>
      <td>2008.0</td>
      <td>250.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DEM</td>
      <td>US</td>
      <td>P</td>
      <td>C</td>
      <td>Q2</td>
      <td>15E</td>
      <td>IND</td>
      <td>IN</td>
      <td>Math-Stat</td>
      <td>2016.0</td>
      <td>250.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>REP</td>
      <td>US</td>
      <td>P</td>
      <td>C</td>
      <td>12G</td>
      <td>15</td>
      <td>IND</td>
      <td>MA</td>
      <td>Engineer</td>
      <td>2016.0</td>
      <td>184.0</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df.cand_pty_affiliation.value_counts(normalize=True).plot(
    kind=&quot;bar&quot;, title=&quot;Share of No. donations&quot;)
plt.show()
</code></pre>
<p><img src="/blog/content/images/2018/01/output_2_0.png" alt="donations"></p>
<p>The figure above is the data underlying Ben's claim. Indeed, between Democrats and Republicans, about 75% of all contributions are made to democrats. Let's go through the features at our disposal. We have data about the donor, the transaction, and the recipient:</p>
<p><img src="/blog/content/images/2018/01/features.svg" alt="features"></p>
<p>To measure how well our models perform, we use the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">ROC-AUC</a> score, which trades off having high precision and high recall (if these concepts are new to you, see the Wikipedia entry on <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision and recall</a> for a quick introduction). If you haven't used this metric before, a random guess has a score of 0.5 and perfect recall and precision yields 1.0.</p>
<h2 id="whatisanensemble">What is an ensemble?</h2>
<p>Imagine that you are playing trivial pursuit. When you play alone, there might be some topics you are good at, and some that you know next to nothing about. If we want to maximize our trivial pursuit score, we need build a team to cover all topics. This is the basic idea of an ensemble: combining predictions from several models averages out idiosyncratic errors and yield better overall predictions.</p>
<p>An important question is how to combine predictions. In our trivial pursuit example, it is easy to imagine that team members might make their case and majority voting decides which to pick. Machine learning is remarkably similar in classification problems: taking the most common class label prediction is equivalent to a majority voting rule. But there are many other ways to combine predictions, and more generally we can use a model to <em>learn</em> how to best combine predictions.</p>
<p><img src="/blog/content/images/2018/01/ensemble_network.png" alt="ensemble network"><br>
<em>Basic ensemble structure. Data is fed to a set of models, and a meta learner combine model predictions. <a href="http://flennerhag.com/2017-04-18-introduction-to-ensembles/">Source</a></em></p>
<h3 id="understandingensemblesbycombiningdecisiontrees">Understanding ensembles by combining decision trees</h3>
<p>To illustrate the machinery of ensembles, we'll start off with a simple interpretable model: a decision tree, which is a tree of <code>if-then</code> rules. If you're unfamiliar with decision trees or would like to dive deeper, check out the <a href="https://www.dataquest.io/course/decision-trees">decision trees course</a> on Dataquest. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.</p>
<p>We'll use the below helper function to visualize our decision rules:</p>
<pre><code class="language-python">import pydotplus  # you can install pydotplus with: pip install pydotplus 
from IPython.display import Image
from sklearn.metrics import roc_auc_score
from sklearn.tree import DecisionTreeClassifier, export_graphviz

def print_graph(clf, feature_names):
    &quot;&quot;&quot;Print decision tree.&quot;&quot;&quot;
    graph = export_graphviz(
        clf,
        label=&quot;root&quot;,
        proportion=True,
        impurity=False, 
        out_file=None, 
        feature_names=feature_names,
        class_names={0: &quot;D&quot;, 1: &quot;R&quot;},
        filled=True,
        rounded=True
    )
    graph = pydotplus.graph_from_dot_data(graph)  
    return Image(graph.create_png())
</code></pre>
<p>Let's fit a decision tree with a single node (decision rule) on our training data and see how it perform on the test set:</p>
<pre><code class="language-python">t1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)
t1.fit(xtrain, ytrain)
p = t1.predict_proba(xtest)[:, 1]

print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
print_graph(t1, xtrain.columns)
</code></pre>
<p><img src="/blog/content/images/2018/01/output_6_1.png" alt="dc1"><br>
<em>Decision tree ROC-AUC score: 0.672</em></p>
<p>Each of the two leaves register their share of training samples, the class distribution within their share, and the class label prediction. Our decision tree bases its prediction on whether the the size of the contribution is above 101.5: but it makes <em>the same</em> prediction regardless! This is not too surprising given that 75% of all donations are to Democrats. But it's not making use of the data we have. Let's use three levels of decision rules and see what we can get:</p>
<pre><code class="language-python">t2 = DecisionTreeClassifier(max_depth=3, random_state=SEED)
t2.fit(xtrain, ytrain)
p = t2.predict_proba(xtest)[:, 1]

print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
print_graph(t2, xtrain.columns)
</code></pre>
<p><img src="/blog/content/images/2018/01/output_8_1.png" alt="dc2"><br>
<em>Decision tree ROC-AUC score: 0.751</em></p>
<p>This model is not much better than the simple decision tree: a measly 5% of all donations are predicted to go to Republicans–far short of the 25% we would expect. A closer look tells us that the decision tree uses some dubious splitting rules. A whopping 47.3% of all observations end up in the left-most leaf, while another 35.9% end up in the leaf second to the right. The vast majority of leaves are therefore irrelevant. Making the model deeper just causes it to overfit.</p>
<p>Fixing depth, a decision tree can be made more complex by increasing &quot;width&quot;, that is, creating several decision trees and combining them. In other words, an ensemble of decision trees. To see why such a model would help, consider how we may force a decision tree to investigate other patterns than those in the above tree. The simplest solution is to remove features that appear early in the tree. Suppose for instance that we remove the transaction amount feature (<code>transaction_amt</code>), the root of the tree. Our new decision tree would look like this:</p>
<pre><code class="language-python">drop = [&quot;transaction_amt&quot;]

xtrain_slim = xtrain.drop(drop, 1)
xtest_slim = xtest.drop(drop, 1)

t3 = DecisionTreeClassifier(max_depth=3, random_state=SEED)
t3.fit(xtrain_slim, ytrain)
p = t3.predict_proba(xtest_slim)[:, 1]


print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
print_graph(t3, xtrain_slim.columns)
</code></pre>
<p><img src="/blog/content/images/2018/01/output_10_1.png" alt="dc3"><br>
<em>Decision tree ROC-AUC score: 0.740</em></p>
<p>The ROC-AUC score is similar, but the share of Republican donation increased to 7.3%. Still too low, but higher than before. Importantly, in contrast to the first tree, where most of the rules related to the transaction itself, this tree is more focused on the residency of the candidate. We now have two models that by themselves have similar predictive power, but operate on different rules. Because of this, they are likely to make different prediction errors, which we can average out with an ensemble.</p>
<h4 id="interludewhyaveragingpredictionswork">Interlude: why averaging predictions work</h4>
<p>Why would we expect averaging predictions to work? Consider a toy example with two observations that we want to generate predictions for. The true label for the first observation is Republican, and the true label for the second observation is Democrat. In this toy example, suppose model 1 is prone to predicting Democrat while model 2 is prone to predicting Republican, as in the below table:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Observation 1</th>
<th>Observation 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>True label</td>
<td>R</td>
<td>D</td>
</tr>
<tr>
<td>Model prediction:        \(P(R)\)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model 1</td>
<td>0.4</td>
<td>0.2</td>
</tr>
<tr>
<td>Model 2</td>
<td>0.8</td>
<td>0.6</td>
</tr>
</tbody>
</table>
<p>If we use the standard 50% cutoff rule for making a class prediction, each decision tree gets one observation right and one wrong. We create an ensemble by averaging the model's class probabilities, which is a majority vote weighted by the strength (probability) of model's prediction. In our toy example, model 2 is certain of its prediction for observation 1, while model 1 is relatively uncertain. Weighting their predictions, the ensemble favors model 2 and correctly predicts Republican. For the second observation, tables are turned and the ensemble correctly predicts Democrat:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Observation 1</th>
<th>Observation 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>True label</td>
<td>R</td>
<td>D</td>
</tr>
<tr>
<td>Ensemble</td>
<td>0.6</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<p>With more than two decision trees, the ensemble predicts in accordance with the majority. For that reason, an ensemble that averages classifier predictions is known as a <strong>majority voting classifier</strong>. When an ensembles averages based on probabilities (as above), we refer to it as <strong>soft voting</strong>, averaging final class label predictions is known as <strong>hard voting</strong>.</p>
<p>Of course, ensembles are no silver bullet. You might have noticed in our toy example that for averaging to work, prediction errors must be <strong>uncorrelated</strong>. If both models made incorrect predictions, the ensemble would not be able to make any corrections. Moreover, in the soft voting scheme, if one model makes an incorrect prediction with a high probability value, the ensemble would be overwhelmed. Generally, ensembles don't get every observation right, but in expectation it will do better than the underlying models.</p>
<h4 id="aforestisanensembleoftrees">A forest is an ensemble of trees</h4>
<p>Returning to our prediction problem, let's see if we can build an ensemble out of our two decision trees. We first check error correlation: highly correlated errors makes for poor ensembles.</p>
<pre><code class="language-python">p1 = t2.predict_proba(xtest)[:, 1]
p2 = t3.predict_proba(xtest_slim)[:, 1]

pd.DataFrame({&quot;full_data&quot;: p1,
              &quot;red_data&quot;: p2}).corr()
</code></pre>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }
<pre><code>.dataframe thead th {
    text-align: left;
}

.dataframe tbody tr th {
    vertical-align: top;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>full_data</th>
      <th>red_data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>full_data</th>
      <td>1.000000</td>
      <td>0.669128</td>
    </tr>
    <tr>
      <th>red_data</th>
      <td>0.669128</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
<p>There is some correlation, but not overly so: there's still a good deal of prediction variance to exploit. To build our first ensemble, we simply average the two model's predictions.</p>
<pre><code class="language-python">p1 = t2.predict_proba(xtest)[:, 1]
p2 = t3.predict_proba(xtest_slim)[:, 1]
p = np.mean([p1, p2], axis=0)
print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
</code></pre>
<p><em>Average of decision tree ROC-AUC score: 0.783</em></p>
<p>Indeed, the ensemble procedure leads to an increased score. But maybe if we had more diverse trees, we could get an even greater gain. How should we choose which features to exclude when designing the decision trees?</p>
<p>A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions. This process is known as <strong>bootstrapped averaging</strong> (often abbreviated <em>bagging</em>), and when applied to decision trees, the resultant model is a <strong>Random Forest</strong>. Let's see what a random forest can do for us. We use the <a href="http://scikit-learn.org/stable/modules/ensemble.html#forest">Scikit-learn</a> implementation and build an ensemble of 10 decision trees, each fitted on a subset of 3 features.</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=10,
    max_features=3,
    random_state=SEED
)

rf.fit(xtrain, ytrain)
p = rf.predict_proba(xtest)[:, 1]
print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
</code></pre>
<p><em>Average of decision tree ROC-AUC score: 0.844</em></p>
<p>The Random Forest yields a significant improvement upon our previous models. We're on to something! But there is only so much you can do with decision trees. It's time we expand our horizon.</p>
<h2 id="ensemblesasaveragedpredictions">Ensembles as averaged predictions</h2>
<p>Our foray into ensembles so far has shown us two important aspects of ensembles:</p>
<ol>
<li>The less correlation in prediction errors, the better</li>
<li>The more models, the better</li>
</ol>
<p>For this reason, it's a good idea to use as different models as possible (as long as they perform decently). So far, we have relied on simple averaging, but later we will see to how use more complex combinations. To keep track of our progress, it is helpful to formalize our ensemble as \(n\) models \(f_i\) averaged into an ensemble \(e\):</p>
<p>\(<br>
e(x) = \frac1n \sum_{i=1}^n f_i(x).<br>
\)</p>
<p>There's no limitation on what models to include: decision trees, linear models, kernel-based models, non-parametric models, neural networks or even other ensembles! Keep in mind though that the more models we include, the slower the ensemble becomes.</p>
<p>To build an ensemble of various models, we begin by benchmarking a set of Scikit-learn classifiers on the dataset. To avoid repeating code, we use the below helper functions:</p>
<pre><code class="language-python"># A host of Scikit-learn models
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.kernel_approximation import Nystroem
from sklearn.kernel_approximation import RBFSampler
from sklearn.pipeline import make_pipeline


def get_models():
    &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot;
    nb = GaussianNB()
    svc = SVC(C=100, probability=True)
    knn = KNeighborsClassifier(n_neighbors=3)
    lr = LogisticRegression(C=100, random_state=SEED)
    nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED)
    gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED)
    rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED)

    models = {'svm': svc,
              'knn': knn,
              'naive bayes': nb,
              'mlp-nn': nn,
              'random forest': rf,
              'gbm': gb,
              'logistic': lr,
              }

    return models


def train_predict(model_list):
    &quot;&quot;&quot;Fit models in list on training set and return preds&quot;&quot;&quot;
    P = np.zeros((ytest.shape[0], len(model_list)))
    P = pd.DataFrame(P)

    print(&quot;Fitting models.&quot;)
    cols = list()
    for i, (name, m) in enumerate(models.items()):
        print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)
        m.fit(xtrain, ytrain)
        P.iloc[:, i] = m.predict_proba(xtest)[:, 1]
        cols.append(name)
        print(&quot;done&quot;)

    P.columns = cols
    print(&quot;Done.\n&quot;)
    return P


def score_models(P, y):
    &quot;&quot;&quot;Score model in prediction DF&quot;&quot;&quot;
    print(&quot;Scoring models.&quot;)
    for m in P.columns:
        score = roc_auc_score(y, P.loc[:, m])
        print(&quot;%-26s: %.3f&quot; % (m, score))
    print(&quot;Done.\n&quot;)
    
</code></pre>
<p>We're now ready to create a prediction matrix \(P\), where each feature corresponds to the predictions made by a given model, and score each model against the test set:</p>
<pre><code class="language-python">models = get_models()
P = train_predict(models)
score_models(P, ytest)
</code></pre>
<table>
<thead>
<tr>
<th>Model</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>svm</td>
<td>0.850</td>
</tr>
<tr>
<td>knn</td>
<td>0.779</td>
</tr>
<tr>
<td>naive bayes</td>
<td>0.803</td>
</tr>
<tr>
<td>mlp-nn</td>
<td>0.851</td>
</tr>
<tr>
<td>random forest</td>
<td>0.844</td>
</tr>
<tr>
<td>gbm</td>
<td>0.878</td>
</tr>
<tr>
<td>logistic</td>
<td>0.854</td>
</tr>
</tbody>
</table>
<p>That's our baseline. The Gradient Boosting Machine (GBM) does best, followed by a simple logistic regression. For our ensemble strategy to work, prediction errors must be relatively uncorrelated. Checking that this holds is our first order of business:</p>
<pre><code class="language-python"># You need ML-Ensemble for this figure: you can install it with: pip install mlens
from mlens.visualization import corrmat

corrmat(P.corr(), inflate=False)
plt.show()
</code></pre>
<p><img src="/blog/content/images/2018/01/output_22_0.png" alt="corr1"></p>
<p>Errors are significantly correlated, which is to be expected for models that perform well, since it's typically the outliers that are hard to get right. Yet most correlations are in the 50-80% span, so there is decent room for improvement. In fact, if we look at error correlations on a class prediction basis things look a bit more promising:</p>
<pre><code class="language-python">corrmat(P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr(), inflate=False)
plt.show()
</code></pre>
<p><img src="/blog/content/images/2018/01/output_24_0.png" alt="corr2"></p>
<p>To create an ensemble, we proceed as before and average predictions, and as we might expect the ensemble outperforms the baseline. Averaging is a simple process, and if we store model predictions, we can start with a simple ensemble and increase its size on the fly as we train new models.</p>
<pre><code class="language-python">print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1)))
</code></pre>
<p><em>Ensemble ROC-AUC score: 0.884</em></p>
<h3 id="visualizinghowensembleswork">Visualizing how ensembles work</h3>
<p>We've understood the power of ensembles as an error correction mechanism. This means that ensembles smooth out decision boundaries by averaging out irregularities. A decision boundary shows us how an estimator carves up feature space into neighborhood within which all observations are predicted to have the same class label. By averaging out base learner decision boundaries, the ensemble is endowed with a smoother boundary that generalize more naturally.</p>
<p>The figure below shows this in action. Here, the example is the iris data set, where the estimators try to classify three types of flowers. The base learners all have some undesirable properties in their boundaries, but the ensemble has a relatively smooth decision boundary that aligns with observations. Amazingly, ensembles both increase model complexity and acts as a regularizer!</p>
<p><img src="/blog/content/images/2018/01/ensemble_decision_regions_2d.jpg" alt="ensemble_decision_regions_2d"><br>
<em>Example decision boundaries for three models and an ensemble of the three. <a href="https://github.com/rasbt/mlxtend">Source</a></em></p>
<p>Another way to understand what is going on in an ensemble when the task is classification, is to inspect the Receiver Operator Curve (ROC). This curve shows us how an estimator trades off precision and recall. Typically, different base learners make different trade offs: some have higher precision by sacrificing recall, and other have higher recall by sacrificing precision.</p>
<p>A non-linear meta learner, on the other hand, is able to, for each training point, adjust which models it relies on. This means that it can significantly reduce necessary sacrifices and retain high precision while increasing recall (or vice versa). In the figure below, the ensemble is making a much smaller sacrifice in precision to increase recall (the ROC is further in the &quot;northeast&quot; corner).</p>
<pre><code class="language-python">from sklearn.metrics import roc_curve

def plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label):
    &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot;
    plt.figure(figsize=(10, 8))
    plt.plot([0, 1], [0, 1], 'k--')
    
    cm = [plt.cm.rainbow(i)
      for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)]
    
    for i in range(P_base_learners.shape[1]):
        p = P_base_learners[:, i]
        fpr, tpr, _ = roc_curve(ytest, p)
        plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1])

    fpr, tpr, _ = roc_curve(ytest, P_ensemble)
    plt.plot(fpr, tpr, label=ens_label, c=cm[0])
        
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title('ROC curve')
    plt.legend(frameon=False)
    plt.show()


plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;)
</code></pre>
<p><img src="/blog/content/images/2018/01/output_28_0.png" alt="roc1"></p>
<h3 id="beyondensemblesasasimpleaverage">Beyond ensembles as a simple average</h3>
<p>But wouldn't you expect more of a boost given the variation in prediction errors? Well, one thing is a bit nagging. Some of the models perform considerably worse than others, yet their influence is just large as better performing models. This can be quite devastating with unbalanced data sets: recall that with soft voting, if a model makes an extreme prediction (i.e close to 0 or 1), that prediction has a strong pull on the prediction average.</p>
<p>An important factor for us is whether models are able to capture the full share of Republican denotations. A simple check shows that all models underrepresent Republican donations, but some are considerably worse than others.</p>
<pre><code class="language-python">p = P.apply(lambda x: 1*(x &gt;= 0.5).value_counts(normalize=True))
p.index = [&quot;DEM&quot;, &quot;REP&quot;]
p.loc[&quot;REP&quot;, :].sort_values().plot(kind=&quot;bar&quot;)
plt.axhline(0.25, color=&quot;k&quot;, linewidth=0.5)
plt.text(0., 0.23, &quot;True share republicans&quot;)
plt.show()
</code></pre>
<p><img src="/blog/content/images/2018/01/output_30_0.png" alt="share rep"></p>
<p>We can try to improve the ensemble by removing the worst offender, say the Multi-Layer Perceptron (MLP):</p>
<pre><code class="language-python">include = [c for c in P.columns if c not in [&quot;mlp-nn&quot;]]
print(&quot;Truncated ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.loc[:, include].mean(axis=1)))
</code></pre>
<p><em>Truncated ensemble ROC-AUC score: 0.883</em></p>
<p>Not really an improvement: we need a smarter way of prioritizing between models. Clearly, removing models from an ensemble is rather drastic as there may be instances where the removed model carried important information. What we really want is to <em>learn</em> a sensible set of weights to use when averaging predictions. This turns the ensemble into a parametric model that needs to be trained.</p>
<h2 id="learningtocombinepredictions">Learning to combine predictions</h2>
<p>Learning a weighted average means that for each model \(f_i\), we have a weight parameter \(\omega_i \in (0, 1)\) that assigns our weight to that model's predictions. Weighted averaging requires all weights to sum to 1. The ensemble is now defined as</p>
<p>\(<br>
e(x) = \sum_{i=1}^n \omega_i \ f_i(x).<br>
\)</p>
<p>This is a minor change from our previous definition, but is interesting since, once the models have generated predictions \(p_i = f_i(x)\), learning the weights is the same as fitting a linear regression on those predictions:</p>
<p>\(<br>
e(p_1, ..., p_n) = \omega_1 p_1 + ... + \ \omega_n p_n,<br>
\)</p>
<p>with some constraints on the weights. Then again, there is no reason to restrict ourself to fitting just a linear model. Suppose instead that we fit a nearest neighbor model. The ensemble would then take local averages based on the nearest neighbors of a given observation, empowering the ensemble to adapt to changes in model performance as the input varies.</p>
<h3 id="implementinganensemble">Implementing an ensemble</h3>
<p>To build this type of ensemble, we need three things:</p>
<ol>
<li>a library of <strong>base learners</strong> that generate predictions</li>
<li>a <strong>meta learner</strong> that learns how to best combine these predictions</li>
<li>a method for splitting the training data between the base learners and the meta learner.</li>
</ol>
<p>Base learners are the ingoing models that take the original input and generate a set of predictions. If we have an original data set ordered as a matrix \(X\) of shape <code>(n_samples, n_features)</code>, the library of base learners output a new prediction matrix \(P_{\text{base}}\) of size <code>(n_samples, n_base_learners)</code>, where each column represent the predictions made by one of the base learners. The meta learner is trained on \(P_{\text{base}}\).</p>
<p>This means that it is absolutely crucial to handle the training set \(X\) in an appropriate way. In particular, if we both train the base learners on \(X\) and have them predict \(X\), the meta learner will be training on the base learner's <em>training error</em>, but at test time it will face their <em>test errors</em>.</p>
<p>We need a strategy for generating a prediction matrix \(P\) that reflects test errors. The simplest strategy is to split the full data set \(X\) in two: train the base learners on one half and have them predict the other half, which then becomes the input to the meta learner. While simple and relatively fast, we loose quite a bit of data. For small and medium sized data sets, the loss of information can be severe, causing the base learners and the meta learner to perform poorly.</p>
<p>To ensure the full data set is covered, we can use <em>cross-validation</em>, a method initially developed for validating test-set performance during model selection. There are many ways to perform cross-validation, and before we delve into that, let's get a feel for this type of ensemble by implementing one ourselves, step by step.</p>
<h4 id="step1definealibraryofbaselearners">Step 1: define a library of base learners</h4>
<p>These are the models that take the raw input data and generates predictions, and can be anything from linear regression to a neural network to another ensemble. As always, there's strength in diversity! The only thing to consider is that the more models we add, the slower the ensemble will be. Here, we'll use our set of models from before:</p>
<pre><code class="language-python">base_learners = get_models()
</code></pre>
<h4 id="step2defineametalearner">Step 2: define a meta learner</h4>
<p>Which meta learner to use is not obvious, but popular choices are linear models, kernel-based models (SVMs and KNNS) and decision tree based models. But you could also use another ensemble as &quot;meta learner&quot;: in this special case, you end up with a two-layer ensemble, akin to a feed-forward neural network.</p>
<p>Here, we'll use a Gradient Boosting Machine. To ensure the GBM explores local patterns, we are restricting each of the 1000 decision trees to train on a random subset of 4 base learners and 50% of input data. This way, the GBM will be exposed to each base learner's strength in different neighborhoods of the input space.</p>
<pre><code class="language-python">meta_learner = GradientBoostingClassifier(
    n_estimators=1000,
    loss=&quot;exponential&quot;,
    max_features=4,
    max_depth=3,
    subsample=0.5,
    learning_rate=0.005, 
    random_state=SEED
)
</code></pre>
<h4 id="step3defineaprocedureforgeneratingtrainandtestsets">Step 3: define a procedure for generating train and test sets</h4>
<p>To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as <strong>Blending</strong>. Unfortunately, the terminology differs between communities, so it's not always easy to know what type of cross-validation the ensemble is using.</p>
<pre><code class="language-python">xtrain_base, xpred_base, ytrain_base, ypred_base = train_test_split(
    xtrain, ytrain, test_size=0.5, random_state=SEED)
</code></pre>
<p>We now have one training set of the base learners \((X_{\text{train_base}}, y_{\text{train_base}})\) and one prediction set \((X_{\text{pred_base}}, y_{\text{pred_base}})\) and are ready to generate the prediction matrix for the meta learner.</p>
<h4 id="step4trainthebaselearnersonatrainingset">Step 4: train the base learners on a training set</h4>
<p>To train the library of base learners on the base-learner training data, we proceed as usual:</p>
<pre><code class="language-python">def train_base_learners(base_learners, inp, out, verbose=True):
    &quot;&quot;&quot;Train all base learners in the library.&quot;&quot;&quot;
    if verbose: print(&quot;Fitting models.&quot;)
    for i, (name, m) in enumerate(base_learners.items()):
        if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)
        m.fit(inp, out)
        if verbose: print(&quot;done&quot;)
</code></pre>
<p>To train the base learners, execute</p>
<pre><code class="language-python">train_base_learners(base_learners, xtrain_base, ytrain_base)
</code></pre>
<h4 id="step5generatebaselearnerpredictions">Step 5: generate base learner predictions</h4>
<p>With the base learners fitted, we can now generate a set of predictions for the meta learner to train on. Note that we generate predictions for observations <em>not</em> used to train the base learners. For each observation \(x_{\text{pred}}^{(i)} \in X_{\text{pred_base}}\) in the base learner prediction set, we generate a set of base learner predictions:</p>
<p>\(<br>
p_{\text{base}}^{(i)} = \left( \ f_1(x_{\text{pred}}^{(i)}) \ , ..., \ f_n(x_{\text{pred}}^{(i)} ) \ \right).<br>
\)</p>
<p>If you implement your own ensemble, pay special attention to how you index the rows and columns of the prediction matrix. When we split the data in two, this is not so hard, but with cross-validation things are more challenging.</p>
<pre><code class="language-python">def predict_base_learners(pred_base_learners, inp, verbose=True):
    &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot;
    P = np.zeros((inp.shape[0], len(pred_base_learners)))

    if verbose: print(&quot;Generating base learner predictions.&quot;)
    for i, (name, m) in enumerate(pred_base_learners.items()):
        if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)
        p = m.predict_proba(inp)
        # With two classes, need only predictions for one class
        P[:, i] = p[:, 1]
        if verbose: print(&quot;done&quot;)

    return P
</code></pre>
<p>To generate predictions, execute</p>
<pre><code class="language-python">P_base = predict_base_learners(base_learners, xpred_base)
</code></pre>
<h4 id="6trainthemetalearner">6. Train the meta learner</h4>
<p>The prediction matrix \(P_{\text{base}}\) reflects test-time performance and can be used to train the meta learner:</p>
<pre><code class="language-python">meta_learner.fit(P_base, ypred_base)
</code></pre>
<p>That's it! We now have a fully trained ensemble that can be used to predict new data. To generate a prediction for some observation \(x^{(j)}\), we first feed it to the base learners. These output a set of predictions</p>
<p>\(<br>
p_{\text{base}}^{(j)} = \left( \ f_1(x^{(j)}) \ , ..., \ f_n(x^{(j)}) \ \right)<br>
\)</p>
<p>that we feed to the meta learner. The meta learner then gives us the ensemble's final prediction</p>
<p>\(<br>
p^{(j)} = m\left(p_{\text{base}}^{(j)} \right).<br>
\)</p>
<p>Now that we have a firm understanding of ensemble learning, it's time to see what it can do to improve our prediction performance on the political contributions data set:</p>
<pre><code class="language-python">def ensemble_predict(base_learners, meta_learner, inp, verbose=True):
    &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot;
    P_pred = predict_base_learners(base_learners, inp, verbose=verbose)
    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]
</code></pre>
<p>To generate predictions, execute</p>
<pre><code class="language-python">P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)
print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
</code></pre>
<p><em>Ensemble ROC-AUC score: 0.881</em></p>
<p>As expected, the ensemble beats the best estimator from our previous benchmark, but it doesn't beat the simple average ensemble. That's because we trained the base learners and the meta learner on only half the data, so a lot of information is lost. To prevent this, we need to use a cross-validation strategy.</p>
<h2 id="trainingwithcrossvalidation">Training with cross-validation</h2>
<p>During cross-validated training of the base learners, a copy of each base learner is fitted on \(K-1\) folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as <strong>stacking</strong>, while the ensemble itself is known as the <strong>Super Learner</strong>.</p>
<p>To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over \(K\) distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here's a simple stacking implementation:</p>
<pre><code class="language-python">from sklearn.base import clone

def stacking(base_learners, meta_learner, X, y, generator):
    &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot;

    # Train final base learners for test time
    print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;)
    train_base_learners(base_learners, X, y, verbose=False)
    print(&quot;done&quot;)

    # Generate predictions for training meta learners
    # Outer loop:
    print(&quot;Generating cross-validated predictions...&quot;)
    cv_preds, cv_y = [], []
    for i, (train_idx, test_idx) in enumerate(generator.split(X)):

        fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx]
        fold_xtest, fold_ytest = X[test_idx, :], y[test_idx]

        # Inner loop: step 4 and 5
        fold_base_learners = {name: clone(model)
                              for name, model in base_learners.items()}
        train_base_learners(
            fold_base_learners, fold_xtrain, fold_ytrain, verbose=False)

        fold_P_base = predict_base_learners(
            fold_base_learners, fold_xtest, verbose=False)

        cv_preds.append(fold_P_base)
        cv_y.append(fold_ytest)
        print(&quot;Fold %i done&quot; % (i + 1))

    print(&quot;CV-predictions done&quot;)
    
    # Be careful to get rows in the right order
    cv_preds = np.vstack(cv_preds)
    cv_y = np.hstack(cv_y)

    # Train meta learner
    print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;)
    meta_learner.fit(cv_preds, cv_y)
    print(&quot;done&quot;)

    return base_learners, meta_learner
</code></pre>
<p>Let's go over the steps involved here. First, we fit our final base learners on <em>all</em> data: in contrast with our previous blend ensemble, base learners used at test time are trained on all available data. We then loop over all folds, then loop over all base learners to generate cross-validated predictions. These predictions are stacked to build the training set for the meta learner, which too sees all data.</p>
<p>The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:</p>
<pre><code class="language-python">from sklearn.model_selection import KFold

# Train with stacking
cv_base_learners, cv_meta_learner = stacking(
    get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))

P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)
print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))
</code></pre>
<p><em>Ensemble ROC-AUC score: 0.889</em></p>
<p>Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly.</p>
<p>Stacking comes with its own set of shortcomings, particularly speed. In general, we need to be aware of there important issues when it comes to implementing ensembles with cross-validation:</p>
<ol>
<li>Computational complexity</li>
<li>Structural complexity (risk of information leakage)</li>
<li>Memory consumption</li>
</ol>
<p>It's important to understand these in order to work with ensembles efficiently, so let's go through each in turn.</p>
<h4 id="1computationalcomplexity">1. Computational complexity</h4>
<p>Suppose we want to use 10 folds for stacking. This would require training all base learners 10 times on 90% of the data, and once on all data. With 4 base learners, the ensemble would roughly be 40 times slower than using the best base learner.</p>
<p>But each cv-fit is independent, so we don't need to fit models sequentially. If we could fit all folds in parallel, the ensemble would only be roughly 4 times slower than the best base learner, a dramatic improvement. Ensembles are prime candidates for <strong>parallelization</strong>, and it is critical to leverage this capability to the greatest extent possible. Fitting all folds for all models in parallel, the time penalty for the ensemble would be negligible. To hone this point in, below is a benchmark from <a href="http://ml-ensemble.com">ML-Ensemble</a> that shows the time it takes to fit an ensemble via stacking or blending either sequentially or in parallel on 4 threads.</p>
<div>
<img src="/blog/content/images/2018/01/scale_cpu-1.png", width=480, height=480>
</div>
<p>Even with this moderate degree of parallelism, we can realize a sizeable reduction in computation time. But parallelization is associated with a whole host of potentially thorny issues such as race conditions, deadlocks and memory explosion.</p>
<h4 id="2structuralcomplexity">2. Structural complexity</h4>
<p>Once we decide to use the entire training set to meta learner, we must worry about <strong>information leakage</strong>. This phenomena arises when we mistakenly predict samples that were used during training, for instance by mixing up our folds or using a model trained on the wrong subset. When there's information leakage in the training set of the meta learner, it will not learn to properly correct for base learner predictions errors: garbage in, garbage out. Spotting such bugs though is extremely difficult.</p>
<h4 id="3memoryconsumption">3. Memory consumption</h4>
<p>The final issue arises with parallelization, especially by multi-processing as is often the case in Python. In this case, each sub-process has its own memory and therefore needs to copy all data from the parent process. A naive implementation will therefore copy all data to all processes, eating up memory and wasting time on data serialization. Preventing this requires sharing data memory, which in turns easily cause data corruption.</p>
<h4 id="upshotusepackages">Upshot: use packages</h4>
<p>The upshot is that you should use a unit-tested package and focus on building your machine learning pipeline. In fact, once you've settled on a ensemble package, building ensembles becomes <em>really</em> easy: all you need to do is specify the base learners, the meta learner, and a method for training the ensemble.</p>
<p>Fortuntately, there are many packages available in all popular programming languages, though they come in different flavors. At the end of this post, we list some as reference. For now, let's pick one and see how a stacked ensemble does on our political contributions data set. Here, we use <a href="http://ml-ensemble.com">ML-Ensemble</a> and build our previous generalized ensemble, but now using 10-fold cross-validation:</p>
<pre><code class="language-python">from mlens.ensemble import SuperLearner

# Instantiate the ensemble with 10 folds
sl = SuperLearner(
    folds=10,
    random_state=SEED,
    verbose=2,
    backend=&quot;multiprocessing&quot;
)

# Add the base learners and the meta learner
sl.add(list(base_learners.values()), proba=True) 
sl.add_meta(meta_learner, proba=True)

# Train the ensemble
sl.fit(xtrain, ytrain)

# Predict the test set
p_sl = sl.predict_proba(xtest)

print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1]))
</code></pre>
<pre><code>Fitting 2 layers
Processing layer-1             done | 00:02:03
Processing layer-2             done | 00:00:03
Fit complete                        | 00:02:08

Predicting 2 layers
Processing layer-1             done | 00:00:50
Processing layer-2             done | 00:00:02
Predict complete                    | 00:00:54

Super Learner ROC-AUC score: 0.890
</code></pre>
<p>It's as simple as that!</p>
<p>Inspecting the ROC-curve of the super learner against the simple average ensemble reveals how leveraging the full data enables the super learner to sacrifice less recall for a given level of precision.</p>
<pre><code class="language-python">plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;)
</code></pre>
<p><img src="/blog/content/images/2018/01/output_59_0.png" alt="output_59_0"></p>
<h2 id="wheretogofromhere">Where to go from here</h2>
<p>There are many other types of ensembles than those presented here. However the basic ingredients are always the same: a library of base learners, a meta learner, and a training procedure. By playing around with these components, various specialized forms of ensembles can be created. A good starting point for more advanced material on ensemble learning is this excellent <a href="https://mlwave.com/kaggle-ensembling-guide/">post</a> by mlware.</p>
<p>When it comes to software, it's a matter of taste. As the popularity of ensembles have risen, so has the number of packages available. Ensembles were traditionally developed in the statistics community, so R has had a lead on purpose-built libraries. Several packages have recently been developed in Python and other languages, with more on the way. Each package caters to different needs and are at different stages of maturity, so we recommend shopping around until you find what you are looking for.</p>
<p>Here are a few packages to get you started:</p>
<table>
<thead>
<tr>
<th>Language</th>
<th>Name</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python</td>
<td><a href="http://ml-ensemble.com">ML-Ensemble</a></td>
<td>General ensemble learning</td>
</tr>
<tr>
<td>Python</td>
<td><a href="http://scikit-learn.org/stable/modules/ensemble.html">Scikit-learn</a></td>
<td>Bagging, majority voting classifiers. API for stacking in development</td>
</tr>
<tr>
<td>Python</td>
<td><a href="http://rasbt.github.io/mlxtend/">mlxtend</a></td>
<td>Regression and Classification ensembles</td>
</tr>
<tr>
<td>R</td>
<td><a href="https://cran.r-project.org/web/packages/SuperLearner/index.html">SuperLearner</a></td>
<td>Super Learner ensembles</td>
</tr>
<tr>
<td>R</td>
<td><a href="https://cran.r-project.org/web/packages/subsemble/index.html">Subsemble</a></td>
<td>Subsembles</td>
</tr>
<tr>
<td>R</td>
<td><a href="https://cran.r-project.org/web/packages/caretEnsemble/index.html">caretEnsemble</a></td>
<td>Ensembles of Caret estimators</td>
</tr>
<tr>
<td>Mutliple</td>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html">H20</a></td>
<td>Distributed stacked ensemble learning. Limited to estimators in the H20 library</td>
</tr>
<tr>
<td>Java</td>
<td><a href="https://github.com/kaz-Anova/StackNet">StackNet</a></td>
<td>Empowered by H20</td>
</tr>
<tr>
<td>Web-based</td>
<td><a href="https://github.com/reiinakano/xcessiv">xcessiv</a></td>
<td>Web-based ensemble learning</td>
</tr>
</tbody>
</table>
<hr>
</div>
            </section>


            <footer class="post-full-footer">
                <div class="newsletter-block">
                    <data data-token="4d32db293dfe16ddcd8c4583e3dcce22" class="mj-w-data" data-apikey="2N0E" data-w-id="68c" data-lang="en_US" data-base="https://app.mailjet.com" data-width="640" data-height="328" data-statics="statics"></data>
                    <div class="mj-w-button mj-w-btn" data-token="4d32db293dfe16ddcd8c4583e3dcce22" style="font-family: Ubuntu, Helvetica; color: white; padding: 0px 25px; background-color: rgb(97, 209, 153); text-align: center; vertical-align: middle; display: inline-block; border-radius: 3px;margin-left:auto;margin-right:auto;">
                        <div style="display: table; height: 45px;">
                            <div style="display: table-cell; vertical-align: middle;">
                                <div class="mj-w-button-content" style="font-family:Ubuntu, Helvetica; display: inline-block; text-align: center; font-size: 13px; vertical-align: middle;" onclick="analytics.track(
                                        'blog-conversion',
                                          {
                                            category: 'email-signup',
                                            label: 'end-post',
                                          });"><b>SUBSCRIBE TO OUR MAILING LIST!</b></div>
                            </div>
                        </div>
                    </div>
                    <script type="text/javascript" src="https://app.mailjet.com/statics/js/widget.modal.js"></script>
                </div>
                <div class="author-block">

                    <section class="author-card">
                            <img class="author-profile-image" src="/blog/content/images/2018/01/Sebastian_Flennerhag-1.jpg" alt="Sebastian Flennerhag" />
                        <section class="author-card-content">
                            <h4 class="author-card-name"><a href="/blog/author/sebastian/">Sebastian Flennerhag</a></h4>
                                <p>Machine learning researcher and predictive modeller. Writes about exciting open source projects, predictive modeling and the latest in deep learning. </p>
                        </section>
                    </section>
                    <div class="post-full-footer-right">
                        <a class="author-card-button" href="/blog/author/sebastian/">Read More</a>
                    </div>
                </div>


            </footer>


        </article>

    </div>
    <div id="ad-box-content" style='display: none;'>
        <h3>Learn data science with Dataquest</h3>
        <p>
            Advance your career with data science and data analysis skills.  Get started for free and join 200,000+ students who've been hired at companies like Amazon, SpaceX, and Microsoft.
        </p>
        <p>
            <a class="learning-cta"
               href="https://www.dataquest.io"
               onclick="analytics.track(
                        'blog-conversion',
                        {
                          category: 'homepage-link',
                          label: 'popup-bottom-right',
                        });"
              >Start Learning</a>
        </p>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card"

                >
                    <header class="read-next-card-header">
                        <small class="read-next-card-header-sitetitle">&mdash; Dataquest &mdash;</small>
                        <h3 class="read-next-card-header-title"><a href="/blog/tag/python/">Python</a></h3>
                    </header>
                    <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                    <div class="read-next-card-content">
                        <ul>
                            <li><a href="/blog/basic-statistics-with-python-descriptive-statistics/">Basic Statistics in Python: Descriptive Statistics</a></li>
                            <li><a href="/blog/python-generators-tutorial/">Python Generators Tutorial</a></li>
                            <li><a href="/blog/programming-best-practices-for-data-science/">Programming Best Practices For Data Science</a></li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="/blog/tag/python/">See all 77 posts →</a>
                    </footer>
                </article>

                <article class="post-card post tag-sql tag-jobs tag-data-science no-image" data-title="Want a Job in Data? Learn This." data-date="2018-01-17">
    <div class="post-card-content">
        <a class="post-card-content-link" href="/blog/why-sql-is-the-most-important-language-to-learn/">
            <header class="post-card-header">
                    <span class="post-card-tags">SQL</span>
                <span class="post-card-date">Jan 17, 2018</span>
                <div style="clear:both;"></div>
                <h2 class="post-card-title">Want a Job in Data? Learn This.</h2>
            </header>
            <section class="post-card-excerpt">
                <p>Ignoring SQL will make it much harder to get a job in data. We tell you why.</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="/blog/content/images/2017/12/josh_profile.png" alt="Josh Devlin" />
            <span class="post-card-author"><a href="/blog/author/josh/">Josh Devlin</a></span>
        </footer>
    </div>
</article>

                <article class="post-card post tag-postgres tag-data-engineering tag-python no-image" data-title="Postgres Internals: Building a Description Tool" data-date="2018-01-10">
    <div class="post-card-content">
        <a class="post-card-content-link" href="/blog/postgres-internals/">
            <header class="post-card-header">
                    <span class="post-card-tags">Postgres</span>
                <span class="post-card-date">Jan 10, 2018</span>
                <div style="clear:both;"></div>
                <h2 class="post-card-title">Postgres Internals: Building a Description Tool</h2>
            </header>
            <section class="post-card-excerpt">
                <p>In this post, learn about Postgres' internal tables and use Python to build your own description dictionary from scratch.</p>
            </section>
        </a>
        <footer class="post-card-meta">
                <img class="author-profile-image" src="/blog/content/images/2017/12/spiro_sideris_profile.jpg" alt="Spiro Sideris" />
            <span class="post-card-author"><a href="/blog/author/spiro/">Spiro Sideris</a></span>
        </footer>
    </div>
</article>

        </div>
    </div>
</aside>

<div class="floating-header">
    <div class="floating-header-logo">
        <a href="https://www.dataquest.io/blog">
                <img src="/blog/content/images/2017/12/ms-icon-144x144.png" alt="Dataquest icon" />
            <span>Dataquest</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Introduction to Python Ensembles</div>
    <div class="floating-header-signup">
        <a
          class="author-card-button"
          href="https://www.dataquest.io"
          onclick="analytics.track(
                        'blog-conversion',
                        {
                          category: 'homepage-link',
                          label: 'floating-header',
                        });"
          >Learn Data Science Online</a>
    </div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Introduction%20to%20Python%20Ensembles&amp;url=https://www.dataquest.io/blog/introduction-to-ensembles/"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://www.dataquest.io/blog/introduction-to-ensembles/"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>





        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://www.dataquest.io/blog">Dataquest</a> &copy; 2018</section>
                <nav class="site-footer-nav">
                    <a href="https://www.dataquest.io/blog">Latest Posts</a>
                    <a href="https://www.facebook.com/dataquestio/" target="_blank" rel="noopener">Facebook</a>
                    <a href="https://twitter.com/dataquestio" target="_blank" rel="noopener">Twitter</a>
                </nav>
            </div>
        </footer>

    </div>


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script type="text/javascript" src="/blog/assets/ghostHunter/dist/jquery.ghosthunter.js?v=85bbdda1fa"></script>
    <script type="text/javascript" src="/blog/assets/js/initghost.js?v=85bbdda1fa"></script>
    <script type="text/javascript" src="/blog/assets/js/jquery.fitvids.js?v=85bbdda1fa"></script>
    <script type="text/javascript" src="/blog/assets/js/fa-solid.min.js?v=85bbdda1fa"></script>
    <script type="text/javascript" src="/blog/assets/js/fontawesome.min.js?v=85bbdda1fa"></script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.8.4/prism.min.js" ></script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/prism/1.8.4/components/prism-python.min.js"></script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/prism/1.8.4/components/prism-r.min.js"></script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/prism/1.8.4/components/prism-sql.min.js"></script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/prism/1.8.4/components/prism-bash.min.js"></script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true,
                processEnvironments: true
            },
            // Center justify equations in code and markdown cells. Elsewhere
            // we use CSS to left justify single line equations in code cells.
            displayAlign: 'center',
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}},
                linebreaks: { automatic: true }
            }
        });
    </script>

    <script>
      if(window.location.host === 'www.dataquest.io') {
        var segment = 'AshBDDdcjA4D5pxKTMHzKN0dCzbjfTc3' // this is the blog prod segment key
      } else {
        var segment = 'Cwx1HZOBtWSKbPh9WcJZyyuo7WAYlEIb' // this is the blog test segment key
      }
      
      !function(){var analytics=window.analytics=window.analytics||[];if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","page","once","off","on"];analytics.factory=function(t){return function(){var e=Array.prototype.slice.call(arguments);e.unshift(t);analytics.push(e);return analytics}};for(var t=0;t<analytics.methods.length;t++){var e=analytics.methods[t];analytics[e]=analytics.factory(e)}analytics.load=function(t){var e=document.createElement("script");e.type="text/javascript";e.async=!0;e.src=("https:"===document.location.protocol?"https://":"http://")+"cdn.segment.com/analytics.js/v1/"+t+"/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n)};analytics.SNIPPET_VERSION="3.1.0";
      analytics.load(segment);
      analytics.page()
      }}();
    </script>


    <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>
<script src="/blog/assets/js/boxzilla.js?v=85bbdda1fa"></script>
<script>
    Boxzilla.init();
    Boxzilla.create('ad-box', {
        content: document.getElementById('ad-box-content').innerHTML,
        trigger: {
            method: 'percentage',
            value: 50
        },
        'cookie': {
            'dismissed': 6,        // hours that a box should stay hidden after being dismissed
            'triggered': 1          // hours that a box should stay hidden after being triggered
        },
        position: "bottom-right"
    });
</script>
<script src="//dqeditor.dataquest.io/dq_post_box.js"></script>


    

</body>
</html>
